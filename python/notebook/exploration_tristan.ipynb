{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'nytf.utils' from '../nytf/utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import datetime, time\n",
    "import random\n",
    "from importlib import reload\n",
    "from math import sqrt\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_paths = {os.pardir}  # package directory\n",
    "sys.path.extend(module_paths.difference(sys.path))\n",
    "\n",
    "\n",
    "from nytf import utils, utils2\n",
    "#from nytf.main_pipeline import main_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 51s, sys: 1min 12s, total: 6min 4s\n",
      "Wall time: 6min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = utils.load_dataframe('train')#, cloud=False)\n",
    "train = pd.concat((train, utils.BasicTemporalFeatures().transform(train)), axis=1)\n",
    "\n",
    "#with open(os.path.join(utils.PROCESSING_DIRECTORY, 'train_basic_features.pkl'), 'rb') as f:\n",
    "#    train = pickle.load(f)\n",
    "\n",
    "test = utils.load_dataframe('test')#, cloud=False)\n",
    "test = pd.concat((test, utils.BasicTemporalFeatures().transform(test)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.402749"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.memory_usage().sum()/10**9  # In Go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "N = 200000\n",
    "ind = sorted(random.sample(range(train.shape[0]), N))\n",
    "X = train.iloc[ind][['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n",
    "           'timestamp', 'week_progress_cos', 'week_progress_sin', 'month_progress_cos', 'month_progress_sin',\n",
    "           'year_progress_cos', 'year_progress_sin', 'day_progress_cos', 'day_progress_sin']].values\n",
    "y = train.iloc[ind]['fare_amount'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_train_est = regr.predict(X_train).clip(0, 100)\n",
    "print('Train error :', sqrt(((y_train_est - y_train) ** 2).mean()))\n",
    "y_test_est = regr.predict(X_test).clip(0, 100)\n",
    "print('Test error :', sqrt(((y_test_est - y_test) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetwork 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = train[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n",
    "           'timestamp', 'week_progress_cos', 'week_progress_sin', 'month_progress_cos', 'month_progress_sin',\n",
    "           'year_progress_cos', 'year_progress_sin', 'day_progress_cos', 'day_progress_sin']].values\n",
    "y = train['fare_amount'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.05)\n",
    "X = scaler.fit_transform(X)\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y_true, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation=lambda tensor: tf.keras.activations.relu(tensor, max_value=100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(0.01), loss='mse', metrics=[rmse])\n",
    "#model.compile(optimizer=tf.keras.optimizers.SGD(0.01), loss='mse', metrics=[rmse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=100, batch_size=2**20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights('test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n",
    "           'timestamp', 'week_progress_cos', 'week_progress_sin', 'month_progress_cos', 'month_progress_sin',\n",
    "           'year_progress_cos', 'year_progress_sin', 'day_progress_cos', 'day_progress_sin']].values\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "plt.hist(y_test_pred, bins=200)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['fare_amount'] = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.to_csv('first_neural_network.csv', index=False, columns=['key', 'fare_amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetwork 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = train[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n",
    "           'timestamp', 'week_progress_cos', 'week_progress_sin', 'month_progress_cos', 'month_progress_sin',\n",
    "           'year_progress_cos', 'year_progress_sin', 'day_progress_cos', 'day_progress_sin']].astype('float32')\n",
    "y = train['fare_amount'].astype('float32').values\n",
    "\n",
    "del train\n",
    "\n",
    "latitude_scaler, longitude_scaler, timestamp_scaler = StandardScaler(), StandardScaler(), StandardScaler()\n",
    "X['pickup_longitude'], X['dropoff_longitude'] = longitude_scaler.fit_transform(X['pickup_longitude'].values.reshape(-1, 1)), longitude_scaler.transform(X['dropoff_longitude'].values.reshape(-1, 1))\n",
    "X['pickup_latitude'], X['dropoff_latitude'] = latitude_scaler.fit_transform(X['pickup_latitude'].values.reshape(-1, 1)), latitude_scaler.transform(X['dropoff_latitude'].values.reshape(-1, 1))\n",
    "X['timestamp'] = timestamp_scaler.fit_transform(X['timestamp'].values.reshape(-1, 1))\n",
    "\n",
    "X_coordinate_pickup = X[['pickup_longitude', 'pickup_latitude']].values\n",
    "X_coordinate_dropoff = X[['dropoff_longitude', 'dropoff_latitude']].values\n",
    "X_other = X.drop(columns=['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']).values\n",
    "\n",
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_coordinate_pickup_train, X_coordinate_pickup_test, X_coordinate_dropoff_train, X_coordinate_dropoff_test, X_other_train, X_other_test, y_train, y_test = train_test_split(X_coordinate_pickup, X_coordinate_dropoff, X_other, y, test_size=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_dim = X_other_train.shape[1]\n",
    "one_coordinate_sequence = [8, 4]\n",
    "two_coordinates_sequence = [16, 32, 16]\n",
    "end_sequence = [64, 64, 32, 16]\n",
    "\n",
    "one_coordinate_model = tf.keras.Sequential()\n",
    "for i, dim in enumerate(one_coordinate_sequence):\n",
    "  if i==0:\n",
    "    one_coordinate_model.add(layers.Dense(dim, input_dim=2, activation='relu'))\n",
    "  else:\n",
    "    one_coordinate_model.add(layers.Dense(dim, activation='relu'))\n",
    "\n",
    "two_coordinates_model = tf.keras.Sequential()\n",
    "for i, dim in enumerate(two_coordinates_sequence):\n",
    "  if i==0:\n",
    "    two_coordinates_model.add(layers.Dense(dim, input_dim=2*one_coordinate_sequence[-1], activation='relu'))\n",
    "  else:\n",
    "    two_coordinates_model.add(layers.Dense(dim, activation='relu'))\n",
    "\n",
    "end_model = tf.keras.Sequential()\n",
    "for i, dim in enumerate(end_sequence):\n",
    "  if i==0:\n",
    "    end_model.add(layers.Dense(dim, input_dim=other_dim+4+two_coordinates_sequence[-1], activation='relu'))\n",
    "  else:\n",
    "    end_model.add(layers.Dense(dim, activation='relu'))\n",
    "\n",
    "final_activation = lambda tensor: tf.keras.activations.relu(tensor, max_value=100)\n",
    "\n",
    "pickup_coordinate_input = layers.Input(shape=(2,), name='pickup_coordinate_input')\n",
    "dropoff_coordinate_input = layers.Input(shape=(2,), name='dropoff_coordinate_input')\n",
    "other_input = layers.Input(shape=(other_dim,), name='other_input')\n",
    "\n",
    "pickup_coordinate_vect = one_coordinate_model(pickup_coordinate_input)\n",
    "dropoff_coordinate_vect = one_coordinate_model(dropoff_coordinate_input)\n",
    "two_coordinates_vect = two_coordinates_model(layers.Concatenate()([pickup_coordinate_vect, dropoff_coordinate_vect]))\n",
    "end_vect = end_model(layers.Concatenate()([two_coordinates_vect, pickup_coordinate_input,\n",
    "                                           dropoff_coordinate_input, other_input]))\n",
    "\n",
    "coordinates_output = layers.Dense(1, activation=final_activation, name='coordinates_output')(two_coordinates_vect)\n",
    "total_output = layers.Dense(1, activation=final_activation, name='total_output')(end_vect)\n",
    "\n",
    "model = tf.keras.Model(inputs=[pickup_coordinate_input, dropoff_coordinate_input, other_input],\n",
    "                       outputs=[total_output, coordinates_output])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y_true, y_pred))))\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.01),\n",
    "              #optimizer=tf.keras.optimizers.SGD(0.01),\n",
    "              loss='mse', metrics=[rmse], loss_weights={'total_output': 1., 'coordinates_output': .2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit({'pickup_coordinate_input': X_coordinate_pickup_train,\n",
    "           'dropoff_coordinate_input': X_coordinate_dropoff_train,\n",
    "           'other_input': X_other_train},\n",
    "          {'total_output': y_train, 'coordinates_output': y_train},\n",
    "          validation_data=({'pickup_coordinate_input': X_coordinate_pickup_test,\n",
    "           'dropoff_coordinate_input': X_coordinate_dropoff_test,\n",
    "           'other_input': X_other_test},\n",
    "          {'total_output': y_test, 'coordinates_output': y_test}),\n",
    "          epochs=100, batch_size=2**19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_test2 = test[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n",
    "           'timestamp', 'week_progress_cos', 'week_progress_sin', 'month_progress_cos', 'month_progress_sin',\n",
    "           'year_progress_cos', 'year_progress_sin', 'day_progress_cos', 'day_progress_sin']].astype('float32')\n",
    "\n",
    "X_test2['pickup_longitude'], X_test2['dropoff_longitude'] = longitude_scaler.transform(X_test2['pickup_longitude'].values.reshape(-1, 1)), longitude_scaler.transform(X_test2['dropoff_longitude'].values.reshape(-1, 1))\n",
    "X_test2['pickup_latitude'], X_test2['dropoff_latitude'] = latitude_scaler.transform(X_test2['pickup_latitude'].values.reshape(-1, 1)), latitude_scaler.transform(X_test2['dropoff_latitude'].values.reshape(-1, 1))\n",
    "X_test2['timestamp'] = timestamp_scaler.transform(X_test2['timestamp'].values.reshape(-1, 1))\n",
    "\n",
    "X_test2_coordinate_pickup = X_test2[['pickup_longitude', 'pickup_latitude']].values\n",
    "X_test2_coordinate_dropoff = X_test2[['dropoff_longitude', 'dropoff_latitude']].values\n",
    "X_test2_other = X_test2.drop(columns=['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetwork3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.9 s, sys: 25.7 s, total: 42.6 s\n",
      "Wall time: 42.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = train[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n",
    "           'timestamp', 'week_progress_cos', 'week_progress_sin', 'month_progress_cos', 'month_progress_sin',\n",
    "           'year_progress_cos', 'year_progress_sin', 'day_progress_cos', 'day_progress_sin']].astype('float32').values\n",
    "\n",
    "X_test = test[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n",
    "           'timestamp', 'week_progress_cos', 'week_progress_sin', 'month_progress_cos', 'month_progress_sin',\n",
    "           'year_progress_cos', 'year_progress_sin', 'day_progress_cos', 'day_progress_sin']].astype('float32').values\n",
    "\n",
    "y = train['fare_amount'].astype('float32').values\n",
    "\n",
    "del train\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 s, sys: 6.31 s, total: 20.3 s\n",
      "Wall time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                240       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 657\n",
      "Trainable params: 657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_activation = lambda tensor: tf.keras.activations.relu(tensor, max_value=100)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_dim=X_test.shape[1]))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(1, activation=final_activation))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y_true, y_pred))))\n",
    "\n",
    "model.compile(\n",
    "  #optimizer=tf.train.AdamOptimizer(0.01),\n",
    "  optimizer=tf.keras.optimizers.SGD(0.005),\n",
    "  loss='mse', metrics=[rmse]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53487215 samples, validate on 540275 samples\n",
      "Epoch 1/100\n",
      "53487215/53487215 [==============================] - 46s 1us/step - loss: 39.1084 - rmse: 5.7905 - val_loss: 18.2808 - val_rmse: 4.2756\n",
      "Epoch 2/100\n",
      "53487215/53487215 [==============================] - 44s 1us/step - loss: 17.7302 - rmse: 4.2105 - val_loss: 17.3706 - val_rmse: 4.1678\n",
      "Epoch 3/100\n",
      "53487215/53487215 [==============================] - 45s 1us/step - loss: 17.2178 - rmse: 4.1493 - val_loss: 17.0191 - val_rmse: 4.1254\n",
      "Epoch 4/100\n",
      "53487215/53487215 [==============================] - 44s 1us/step - loss: 16.9192 - rmse: 4.1132 - val_loss: 16.7610 - val_rmse: 4.0940\n",
      "Epoch 5/100\n",
      "53487215/53487215 [==============================] - 43s 1us/step - loss: 16.6837 - rmse: 4.0845 - val_loss: 16.5819 - val_rmse: 4.0721\n",
      "Epoch 6/100\n",
      "53487215/53487215 [==============================] - 44s 1us/step - loss: 16.4792 - rmse: 4.0594 - val_loss: 16.3569 - val_rmse: 4.0444\n",
      "Epoch 7/100\n",
      "53487215/53487215 [==============================] - 44s 1us/step - loss: 16.2813 - rmse: 4.0349 - val_loss: 16.1839 - val_rmse: 4.0229\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-453bda112908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   epochs=100, batch_size=2**19)\n\u001b[0m",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1214\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mindex_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_shuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m       \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "  X_train, y_train,\n",
    "  validation_data=(X_valid, y_valid),\n",
    "  epochs=100, batch_size=2**19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6f00c269e103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# 14\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhidden_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfinal_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "input_dim = X_test.shape[1]  # 14\n",
    "hidden_dims = [16, 8, 4]\n",
    "\n",
    "final_activation = lambda tensor: tf.keras.activations.relu(tensor, max_value=100)\n",
    "\n",
    "model_input = layers.Input(shape=(input_dim,), name='input')\n",
    "outputs = []\n",
    "\n",
    "last_tensor = model_input\n",
    "for i in range(N):\n",
    "    for dim in hidden_dims:\n",
    "        last_tensor = layers.Dense(dim, activation='relu')(last_tensor)\n",
    "    outputs.append(layers.Dense(1, activation=final_activation, name='output_{}'.format(i + 1))(last_tensor))\n",
    "    last_tensor = layers.Concatenate()([model_input, last_tensor])\n",
    "\n",
    "model = tf.keras.Model(inputs=model_input, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y_true, y_pred))))\n",
    "\n",
    "model.compile(\n",
    "  optimizer=tf.train.AdamOptimizer(0.001),\n",
    "  #optimizer=tf.keras.optimizers.SGD(0.01),\n",
    "  #optimizer=tf.train.AdadeltaOptimizer(),\n",
    "  loss='mse', metrics=[rmse]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53487215 samples, validate on 540275 samples\n",
      "Epoch 1/100\n",
      "53487215/53487215 [==============================] - 319s 6us/step - loss: 120.3263 - output_1_loss: 13.7183 - output_2_loss: 13.2332 - output_3_loss: 12.8329 - output_4_loss: 11.9842 - output_5_loss: 11.8620 - output_6_loss: 11.4726 - output_7_loss: 11.3846 - output_8_loss: 11.3062 - output_9_loss: 11.2717 - output_10_loss: 11.2606 - output_1_rmse: 3.7031 - output_2_rmse: 3.6370 - output_3_rmse: 3.5815 - output_4_rmse: 3.4612 - output_5_rmse: 3.4435 - output_6_rmse: 3.3866 - output_7_rmse: 3.3736 - output_8_rmse: 3.3620 - output_9_rmse: 3.3568 - output_10_rmse: 3.3552 - val_loss: 123.2512 - val_output_1_loss: 14.0968 - val_output_2_loss: 13.6241 - val_output_3_loss: 13.2245 - val_output_4_loss: 12.2213 - val_output_5_loss: 12.1098 - val_output_6_loss: 11.7390 - val_output_7_loss: 11.6412 - val_output_8_loss: 11.5510 - val_output_9_loss: 11.5238 - val_output_10_loss: 11.5196 - val_output_1_rmse: 3.7542 - val_output_2_rmse: 3.6907 - val_output_3_rmse: 3.6362 - val_output_4_rmse: 3.4952 - val_output_5_rmse: 3.4792 - val_output_6_rmse: 3.4255 - val_output_7_rmse: 3.4112 - val_output_8_rmse: 3.3980 - val_output_9_rmse: 3.3940 - val_output_10_rmse: 3.3934\n",
      "Epoch 2/100\n",
      "53487215/53487215 [==============================] - 317s 6us/step - loss: 120.3074 - output_1_loss: 13.7233 - output_2_loss: 13.2316 - output_3_loss: 12.8311 - output_4_loss: 11.9828 - output_5_loss: 11.8594 - output_6_loss: 11.4698 - output_7_loss: 11.3822 - output_8_loss: 11.3027 - output_9_loss: 11.2677 - output_10_loss: 11.2567 - output_1_rmse: 3.7037 - output_2_rmse: 3.6368 - output_3_rmse: 3.5813 - output_4_rmse: 3.4610 - output_5_rmse: 3.4431 - output_6_rmse: 3.3862 - output_7_rmse: 3.3732 - output_8_rmse: 3.3614 - output_9_rmse: 3.3562 - output_10_rmse: 3.3546 - val_loss: 123.0909 - val_output_1_loss: 14.0866 - val_output_2_loss: 13.6126 - val_output_3_loss: 13.2027 - val_output_4_loss: 12.2020 - val_output_5_loss: 12.0927 - val_output_6_loss: 11.7249 - val_output_7_loss: 11.6291 - val_output_8_loss: 11.5394 - val_output_9_loss: 11.5060 - val_output_10_loss: 11.4949 - val_output_1_rmse: 3.7528 - val_output_2_rmse: 3.6891 - val_output_3_rmse: 3.6332 - val_output_4_rmse: 3.4924 - val_output_5_rmse: 3.4768 - val_output_6_rmse: 3.4234 - val_output_7_rmse: 3.4094 - val_output_8_rmse: 3.3962 - val_output_9_rmse: 3.3913 - val_output_10_rmse: 3.3897\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "  X_train, [y_train for _ in range(N)],\n",
    "  validation_data=(X_valid, [y_valid for _ in range(N)]),\n",
    "  epochs=100, batch_size=2**16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/matplotlib/font_manager.py:1320: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c5761ad30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test = model.predict(X_test)[-1]\n",
    "plt.hist(y_test, bins=200)\n",
    "test['fare_amount'] = y_test\n",
    "#test.to_csv('deep_neural_network1.csv', index=False, columns=['key', 'fare_amount'])\n",
    "#model.save('deep_neural_network1.h5')\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
