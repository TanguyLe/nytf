{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import datetime, time\n",
    "import random\n",
    "from importlib import reload\n",
    "from math import sqrt\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_paths = {os.pardir}  # package directory\n",
    "sys.path.extend(module_paths.difference(sys.path))\n",
    "\n",
    "\n",
    "from nytf import utils, utils2\n",
    "#from nytf.main_pipeline import main_estimator\n",
    "\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = utils.load_dataframe('train_total_features', cloud=False)\n",
    "#train = pd.concat((train, utils.BasicTemporalFeatures().transform(train)), axis=1)\n",
    "\n",
    "#with open(os.path.join(utils.PROCESSING_DIRECTORY, 'train_basic_features.pkl'), 'rb') as f:\n",
    "#    train = pickle.load(f)\n",
    "\n",
    "test = utils.load_dataframe('test_total_features', cloud=False)\n",
    "#test = pd.concat((test, utils.BasicTemporalFeatures().transform(test)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.memory_usage().sum()/10**9  # In Go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "N = 200000\n",
    "ind = sorted(random.sample(range(train.shape[0]), N))\n",
    "X = train.iloc[ind][['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n",
    "           'timestamp', 'week_progress_cos', 'week_progress_sin', 'month_progress_cos', 'month_progress_sin',\n",
    "           'year_progress_cos', 'year_progress_sin', 'day_progress_cos', 'day_progress_sin']].values\n",
    "y = train.iloc[ind]['fare_amount'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_train_est = regr.predict(X_train).clip(0, 100)\n",
    "print('Train error :', sqrt(((y_train_est - y_train) ** 2).mean()))\n",
    "y_test_est = regr.predict(X_test).clip(0, 100)\n",
    "print('Test error :', sqrt(((y_test_est - y_test) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetwork 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = train[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n",
    "           'timestamp', 'week_progress_cos', 'week_progress_sin', 'month_progress_cos', 'month_progress_sin',\n",
    "           'year_progress_cos', 'year_progress_sin', 'day_progress_cos', 'day_progress_sin']].values\n",
    "y = train['fare_amount'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.05)\n",
    "X = scaler.fit_transform(X)\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y_true, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation=lambda tensor: tf.keras.activations.relu(tensor, max_value=100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(0.01), loss='mse', metrics=[rmse])\n",
    "#model.compile(optimizer=tf.keras.optimizers.SGD(0.01), loss='mse', metrics=[rmse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=100, batch_size=2**20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights('test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n",
    "           'timestamp', 'week_progress_cos', 'week_progress_sin', 'month_progress_cos', 'month_progress_sin',\n",
    "           'year_progress_cos', 'year_progress_sin', 'day_progress_cos', 'day_progress_sin']].values\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "plt.hist(y_test_pred, bins=200)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['fare_amount'] = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.to_csv('first_neural_network.csv', index=False, columns=['key', 'fare_amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetwork 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = train[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n",
    "           'timestamp', 'week_progress_cos', 'week_progress_sin', 'month_progress_cos', 'month_progress_sin',\n",
    "           'year_progress_cos', 'year_progress_sin', 'day_progress_cos', 'day_progress_sin']].astype('float32')\n",
    "y = train['fare_amount'].astype('float32').values\n",
    "\n",
    "del train\n",
    "\n",
    "latitude_scaler, longitude_scaler, timestamp_scaler = StandardScaler(), StandardScaler(), StandardScaler()\n",
    "X['pickup_longitude'], X['dropoff_longitude'] = longitude_scaler.fit_transform(X['pickup_longitude'].values.reshape(-1, 1)), longitude_scaler.transform(X['dropoff_longitude'].values.reshape(-1, 1))\n",
    "X['pickup_latitude'], X['dropoff_latitude'] = latitude_scaler.fit_transform(X['pickup_latitude'].values.reshape(-1, 1)), latitude_scaler.transform(X['dropoff_latitude'].values.reshape(-1, 1))\n",
    "X['timestamp'] = timestamp_scaler.fit_transform(X['timestamp'].values.reshape(-1, 1))\n",
    "\n",
    "X_coordinate_pickup = X[['pickup_longitude', 'pickup_latitude']].values\n",
    "X_coordinate_dropoff = X[['dropoff_longitude', 'dropoff_latitude']].values\n",
    "X_other = X.drop(columns=['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']).values\n",
    "\n",
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_coordinate_pickup_train, X_coordinate_pickup_test, X_coordinate_dropoff_train, X_coordinate_dropoff_test, X_other_train, X_other_test, y_train, y_test = train_test_split(X_coordinate_pickup, X_coordinate_dropoff, X_other, y, test_size=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_dim = X_other_train.shape[1]\n",
    "one_coordinate_sequence = [8, 4]\n",
    "two_coordinates_sequence = [16, 32, 16]\n",
    "end_sequence = [64, 64, 32, 16]\n",
    "\n",
    "one_coordinate_model = tf.keras.Sequential()\n",
    "for i, dim in enumerate(one_coordinate_sequence):\n",
    "  if i==0:\n",
    "    one_coordinate_model.add(layers.Dense(dim, input_dim=2, activation='relu'))\n",
    "  else:\n",
    "    one_coordinate_model.add(layers.Dense(dim, activation='relu'))\n",
    "\n",
    "two_coordinates_model = tf.keras.Sequential()\n",
    "for i, dim in enumerate(two_coordinates_sequence):\n",
    "  if i==0:\n",
    "    two_coordinates_model.add(layers.Dense(dim, input_dim=2*one_coordinate_sequence[-1], activation='relu'))\n",
    "  else:\n",
    "    two_coordinates_model.add(layers.Dense(dim, activation='relu'))\n",
    "\n",
    "end_model = tf.keras.Sequential()\n",
    "for i, dim in enumerate(end_sequence):\n",
    "  if i==0:\n",
    "    end_model.add(layers.Dense(dim, input_dim=other_dim+4+two_coordinates_sequence[-1], activation='relu'))\n",
    "  else:\n",
    "    end_model.add(layers.Dense(dim, activation='relu'))\n",
    "\n",
    "final_activation = lambda tensor: tf.keras.activations.relu(tensor, max_value=100)\n",
    "\n",
    "pickup_coordinate_input = layers.Input(shape=(2,), name='pickup_coordinate_input')\n",
    "dropoff_coordinate_input = layers.Input(shape=(2,), name='dropoff_coordinate_input')\n",
    "other_input = layers.Input(shape=(other_dim,), name='other_input')\n",
    "\n",
    "pickup_coordinate_vect = one_coordinate_model(pickup_coordinate_input)\n",
    "dropoff_coordinate_vect = one_coordinate_model(dropoff_coordinate_input)\n",
    "two_coordinates_vect = two_coordinates_model(layers.Concatenate()([pickup_coordinate_vect, dropoff_coordinate_vect]))\n",
    "end_vect = end_model(layers.Concatenate()([two_coordinates_vect, pickup_coordinate_input,\n",
    "                                           dropoff_coordinate_input, other_input]))\n",
    "\n",
    "coordinates_output = layers.Dense(1, activation=final_activation, name='coordinates_output')(two_coordinates_vect)\n",
    "total_output = layers.Dense(1, activation=final_activation, name='total_output')(end_vect)\n",
    "\n",
    "model = tf.keras.Model(inputs=[pickup_coordinate_input, dropoff_coordinate_input, other_input],\n",
    "                       outputs=[total_output, coordinates_output])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y_true, y_pred))))\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.01),\n",
    "              #optimizer=tf.keras.optimizers.SGD(0.01),\n",
    "              loss='mse', metrics=[rmse], loss_weights={'total_output': 1., 'coordinates_output': .2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit({'pickup_coordinate_input': X_coordinate_pickup_train,\n",
    "           'dropoff_coordinate_input': X_coordinate_dropoff_train,\n",
    "           'other_input': X_other_train},\n",
    "          {'total_output': y_train, 'coordinates_output': y_train},\n",
    "          validation_data=({'pickup_coordinate_input': X_coordinate_pickup_test,\n",
    "           'dropoff_coordinate_input': X_coordinate_dropoff_test,\n",
    "           'other_input': X_other_test},\n",
    "          {'total_output': y_test, 'coordinates_output': y_test}),\n",
    "          epochs=100, batch_size=2**19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_test2 = test[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n",
    "           'timestamp', 'week_progress_cos', 'week_progress_sin', 'month_progress_cos', 'month_progress_sin',\n",
    "           'year_progress_cos', 'year_progress_sin', 'day_progress_cos', 'day_progress_sin']].astype('float32')\n",
    "\n",
    "X_test2['pickup_longitude'], X_test2['dropoff_longitude'] = longitude_scaler.transform(X_test2['pickup_longitude'].values.reshape(-1, 1)), longitude_scaler.transform(X_test2['dropoff_longitude'].values.reshape(-1, 1))\n",
    "X_test2['pickup_latitude'], X_test2['dropoff_latitude'] = latitude_scaler.transform(X_test2['pickup_latitude'].values.reshape(-1, 1)), latitude_scaler.transform(X_test2['dropoff_latitude'].values.reshape(-1, 1))\n",
    "X_test2['timestamp'] = timestamp_scaler.transform(X_test2['timestamp'].values.reshape(-1, 1))\n",
    "\n",
    "X_test2_coordinate_pickup = X_test2[['pickup_longitude', 'pickup_latitude']].values\n",
    "X_test2_coordinate_dropoff = X_test2[['dropoff_longitude', 'dropoff_latitude']].values\n",
    "X_test2_other = X_test2.drop(columns=['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetwork3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = train[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n",
    "           'timestamp', 'week_progress_cos', 'week_progress_sin', 'month_progress_cos', 'month_progress_sin',\n",
    "           'year_progress_cos', 'year_progress_sin', 'day_progress_cos', 'day_progress_sin']].astype('float32').values\n",
    "\n",
    "X_test = test[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n",
    "           'timestamp', 'week_progress_cos', 'week_progress_sin', 'month_progress_cos', 'month_progress_sin',\n",
    "           'year_progress_cos', 'year_progress_sin', 'day_progress_cos', 'day_progress_sin']].astype('float32').values\n",
    "\n",
    "y = train['fare_amount'].astype('float32').values\n",
    "\n",
    "del train\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_activation = lambda tensor: tf.keras.activations.relu(tensor, max_value=100)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_dim=X_test.shape[1]))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(1, activation=final_activation))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y_true, y_pred))))\n",
    "\n",
    "model.compile(\n",
    "  #optimizer=tf.train.AdamOptimizer(0.01),\n",
    "  optimizer=tf.keras.optimizers.SGD(0.005),\n",
    "  loss='mse', metrics=[rmse]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "  X_train, y_train,\n",
    "  validation_data=(X_valid, y_valid),\n",
    "  epochs=100, batch_size=2**19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 7\n",
    "input_dim = X_test.shape[1]  # 14\n",
    "hidden_dims = [32, 64, 16]\n",
    "\n",
    "final_activation = lambda tensor: tf.keras.activations.relu(tensor, max_value=100)\n",
    "\n",
    "model_input = layers.Input(shape=(input_dim,), name='input')\n",
    "outputs = []\n",
    "\n",
    "last_tensor = model_input\n",
    "for i in range(N):\n",
    "    for dim in hidden_dims:\n",
    "        last_tensor = layers.Dense(dim, activation='relu')(last_tensor)\n",
    "    outputs.append(layers.Dense(1, activation=final_activation, name='output_{}'.format(i + 1))(last_tensor))\n",
    "    last_tensor = layers.Concatenate()([model_input, last_tensor])\n",
    "\n",
    "model = tf.keras.Model(inputs=model_input, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y_true, y_pred))))\n",
    "\n",
    "model.compile(\n",
    "  optimizer=tf.train.AdamOptimizer(0.001),\n",
    "  #optimizer=tf.keras.optimizers.SGD(0.001, nesterov=True),\n",
    "  #optimizer=tf.train.AdadeltaOptimizer(),\n",
    "  loss='mse', metrics=[rmse]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "  X_train, [y_train for _ in range(N)],\n",
    "  validation_data=(X_valid, [y_valid for _ in range(N)]),\n",
    "  epochs=100, batch_size=2**19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['fare_amount'] = model.predict(X_test)[-1]\n",
    "plt.hist(test['fare_amount'], bins=100)\n",
    "pass\n",
    "#test.to_csv('deep_nn_2.csv', index=False, columns=['key', 'fare_amount'])\n",
    "#model.save('deep_nn_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetork4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'latitude_diff', 'longitude_diff', 'distance2', 'distance2_jfk', 'distance2_guardia',\n",
    "           'passenger_count', 'timestamp', 'year_progress_cos', 'year_progress_sin', 'week_progress_cos', 'week_progress_sin', 'month_progress_cos', 'month_progress_sin',\n",
    "           'day_progress_cos', 'day_progress_sin', 'holiday_score', 'night_hour', 'peak_hour']\n",
    "\n",
    "X = train[columns].astype('float32').values\n",
    "X_test = test[columns].astype('float32').values\n",
    "y = train['fare_amount'].astype('float32').values\n",
    "\n",
    "del train\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_activation = lambda tensor: tf.keras.activations.relu(tensor, max_value=100)\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y_true, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_dim=X_test.shape[1]))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(1, activation=final_activation))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.train.AdamOptimizer(),\n",
    "  #optimizer=tf.keras.optimizers.SGD(0.005),\n",
    "  loss='mse', metrics=[rmse]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "  X_train, y_train,\n",
    "  validation_data=(X_valid, y_valid),\n",
    "  epochs=100, batch_size=2**19\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "input_dim = X_test.shape[1]  # 22\n",
    "hidden_dims = [32, 32, 32, 8]\n",
    "\n",
    "model_input = layers.Input(shape=(input_dim,), name='input')\n",
    "outputs = []\n",
    "\n",
    "last_tensor = model_input\n",
    "for i in range(N):\n",
    "    for dim in hidden_dims:\n",
    "        last_tensor = layers.Dense(dim, activation='relu')(last_tensor)\n",
    "    outputs.append(layers.Dense(1, activation=final_activation, name='output_{}'.format(i + 1))(last_tensor))\n",
    "    last_tensor = layers.Concatenate()([model_input, *outputs, last_tensor])\n",
    "\n",
    "model = tf.keras.Model(inputs=model_input, outputs=outputs)\n",
    "model.load_weights('deep_nn_3.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.train.AdamOptimizer(0.0002),\n",
    "  #optimizer=tf.keras.optimizers.SGD(0.001, nesterov=True),\n",
    "  #optimizer=tf.train.AdadeltaOptimizer(0.001),\n",
    "  loss='mse', metrics=[rmse]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint('deep_nn_3.h5', period=10)\n",
    "\n",
    "model.fit(\n",
    "  X_train, [y_train for _ in range(N)],\n",
    "  validation_data=(X_valid, [y_valid for _ in range(N)]),\n",
    "  epochs=100, batch_size=2**17)#, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('deep_nn_3.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
